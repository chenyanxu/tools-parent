input {
    #file可以多次使用，也可以只写一个file而设置它的path属性配置多个文件实现多文件监控
    file {
        #path为上传文件的路径
        path => "D:/Downloads/logstash-6.5.4/var/logs/karaf-kalix.log"
        #type是给结果增加了一个属性叫type值为"<xxx>"的条目。这里的type，对应了ES中index中的type，即如果输入ES时，没有指定type，那么这里的type将作为ES中index的type
        type => "kalix"
        #start_position可以设置为beginning或者end，beginning表示从头开始读取文件，end表示读取最新的，这个也要和ignore_older一起使用
        start_position => "beginning"
        discover_interval => 15
        #multiline多行匹配
        codec => multiline {
            #pattern为指定的开头
            pattern => "^%{TIMESTAMP_ISO8601} "
            negate => true
            #what有两个值可选 previous和next，举例说明，java的异常从第二行以空格开始，这里就可以pattern匹配空格开始，what设置为previous意思是空格开头这行跟上一行属于同一event。另一个例子，有时候一条命令太长，当以\结尾时表示这行属于跟下一行属于同一event，这时需要使用negate=>true，what=>'next'
            what => "previous"
    }
    #这个表示关闭超过（默认）3600秒后追踪文件。这个对于multiline来说特别有用。... 这个参数和logstash对文件的读取方式有关，两种方式read tail，如果是read
    close_older => 3600
    #ignore_older表示了针对多久的文件进行监控，默认一天，单位为秒，可以自己定制，比如默认只读取一天内被修改的文件
    ignore_older => 86400
    }
    stdin{
    }
}

filter{
    grok{
        #匹配正则表达式
        match => ["message","%{TIMESTAMP_ISO8601:logtime} \|%{GREEDYDATA:level}\|%{GREEDYDATA:threadName}\|%{GREEDYDATA:loggerName}\|%{GREEDYDATA:bundlename}\|%{GREEDYDATA:Message}"]
    }
    mutate {
        #删除某个字段
        remove_field => ["message"]
    }
}

output {
    #上传到elasticsearch
    elasticsearch {
        #上传的类型，index：索引文档（来自Logstash的事件）、delete：按id删除文档（此操作需要id）、create：索引文档，如果索引中已存在该id的文档，则会失败、update：按ID更新文档
        action => "index"
        #index名字，%{+YYYY.MM.dd}为自动添加当前时间
        index => "karaf-kalix-%{+YYYY.MM.dd}"
        #elasticsearch的ip地址
        hosts => "localhost:9200"
        #默认模板名称
        manage_template => true
        #自己定义模板
        template_overwrite => true
    }
    stdout {
        #输出到控制台
        codec => rubydebug
    }
}